"""
Pure CuTile Multi-Head Attention Implementation
End-to-end implementation using only NVIDIA CuTile kernels
"""

import cuda.tile as ct
import cupy as cp
import numpy as np
from typing import Optional, Tuple


def get_stream():
    """Get current CUDA stream pointer."""
    return cp.cuda.get_current_stream().ptr


# ============================================================================
# CuTile Kernels for Attention
# ============================================================================

@ct.kernel
def attention_scores_kernel(
    q,              # Query: (batch_heads, seq_q, head_dim)
    k,              # Key: (batch_heads, seq_k, head_dim)
    scores,         # Output: (batch_heads, seq_q, seq_k)
    scale: ct.Constant[float],
    seq_k: ct.Constant[int],
    head_dim: ct.Constant[int]
):
    """
    Compute scaled attention scores for a single query position.
    Grid: (batch_heads, seq_q)
    """
    pid_bh = ct.bid(0)
    pid_q = ct.bid(1)

    # Load query vector: (head_dim,)
    q_tile = ct.load(q, index=(pid_bh, pid_q, 0), shape=(1, 1, head_dim))
    q_tile = ct.reshape(q_tile, (1, head_dim))

    # Load all keys for this batch_head: (seq_k, head_dim)
    k_tile = ct.load(k, index=(pid_bh, 0, 0), shape=(1, seq_k, head_dim))
    k_tile = ct.reshape(k_tile, (seq_k, head_dim))

    # Compute Q @ K^T: (1, head_dim) @ (head_dim, seq_k) = (1, seq_k)
    k_t = ct.transpose(k_tile)  # (head_dim, seq_k)
    scores_tile = ct.matmul(q_tile, k_t)  # (1, seq_k)

    # Scale
    scores_tile = scores_tile * scale

    # Reshape to match 3D output array and store
    scores_tile = ct.reshape(scores_tile, (1, 1, seq_k))
    ct.store(scores, index=(pid_bh, pid_q, 0), tile=scores_tile)


@ct.kernel
def softmax_inplace_kernel(
    scores,         # Input/Output: (batch_heads, seq_q, seq_k)
    seq_k: ct.Constant[int]
):
    """
    Apply softmax along the last dimension (seq_k).
    Grid: (batch_heads, seq_q)
    """
    pid_bh = ct.bid(0)
    pid_q = ct.bid(1)

    # Load scores for this position
    s_tile = ct.load(scores, index=(pid_bh, pid_q, 0), shape=(1, 1, seq_k))
    s_tile = ct.reshape(s_tile, (seq_k,))

    # Softmax: exp(x - max) / sum(exp(x - max))
    max_val = ct.max(s_tile)
    s_tile = s_tile - max_val
    exp_tile = ct.exp(s_tile)
    sum_val = ct.sum(exp_tile)
    softmax_tile = exp_tile / sum_val

    # Store back
    softmax_tile = ct.reshape(softmax_tile, (1, 1, seq_k))
    ct.store(scores, index=(pid_bh, pid_q, 0), tile=softmax_tile)


@ct.kernel
def attention_output_kernel(
    attn_weights,   # Attention weights: (batch_heads, seq_q, seq_k)
    v,              # Values: (batch_heads, seq_k, head_dim)
    output,         # Output: (batch_heads, seq_q, head_dim)
    seq_k: ct.Constant[int],
    head_dim: ct.Constant[int]
):
    """
    Compute attention output: attn_weights @ V
    Grid: (batch_heads, seq_q)
    """
    pid_bh = ct.bid(0)
    pid_q = ct.bid(1)

    # Load attention weights for this query position: (1, seq_k)
    w_tile = ct.load(attn_weights, index=(pid_bh, pid_q, 0), shape=(1, 1, seq_k))
    w_tile = ct.reshape(w_tile, (1, seq_k))

    # Load all values for this batch_head: (seq_k, head_dim)
    v_tile = ct.load(v, index=(pid_bh, 0, 0), shape=(1, seq_k, head_dim))
    v_tile = ct.reshape(v_tile, (seq_k, head_dim))

    # Compute weighted sum: (1, seq_k) @ (seq_k, head_dim) = (1, head_dim)
    out_tile = ct.matmul(w_tile, v_tile)

    # Store
    out_tile = ct.reshape(out_tile, (1, 1, head_dim))
    ct.store(output, index=(pid_bh, pid_q, 0), tile=out_tile)


@ct.kernel
def causal_mask_kernel(
    scores,         # Scores: (batch_heads, seq_q, seq_k)
    seq_k: ct.Constant[int],
    offset: ct.Constant[int]  # For KV cache: offset in sequence
):
    """
    Apply causal mask to attention scores.
    Grid: (batch_heads, seq_q)
    Sets scores[i, j] = -inf where j > i + offset
    """
    pid_bh = ct.bid(0)
    pid_q = ct.bid(1)

    # Load scores
    s_tile = ct.load(scores, index=(pid_bh, pid_q, 0), shape=(1, 1, seq_k))
    s_tile = ct.reshape(s_tile, (seq_k,))

    # Create mask: positions > (pid_q + offset) should be masked
    positions = ct.arange(seq_k, dtype=ct.int32)
    mask_threshold = pid_q + offset

    # Create mask (1 where valid, 0 where masked)
    # CuTile doesn't have direct comparison ops, use arithmetic
    # For causal: position <= query_pos + offset is valid
    # This is tricky without comparison ops - we'll use a different approach

    # Store (without mask for now - mask applied in Python wrapper)
    s_tile = ct.reshape(s_tile, (1, 1, seq_k))
    ct.store(scores, index=(pid_bh, pid_q, 0), tile=s_tile)


# ============================================================================
# Attention Classes
# ============================================================================

class MultiHeadAttention:
    """Multi-head attention using pure CuTile kernels."""

    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        num_kv_heads: Optional[int] = None,
        head_dim: Optional[int] = None
    ):
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.num_kv_heads = num_kv_heads or num_heads
        self.head_dim = head_dim or (hidden_size // num_heads)
        self.scale = 1.0 / np.sqrt(self.head_dim)

        # Number of query heads per KV head (for GQA)
        self.num_queries_per_kv = self.num_heads // self.num_kv_heads

    def __call__(
        self,
        q: cp.ndarray,
        k: cp.ndarray,
        v: cp.ndarray,
        attention_mask: Optional[cp.ndarray] = None,
        is_causal: bool = False
    ) -> cp.ndarray:
        """
        Compute multi-head attention.

        Args:
            q: Query (batch, num_heads, seq_q, head_dim)
            k: Key (batch, num_kv_heads, seq_k, head_dim)
            v: Value (batch, num_kv_heads, seq_k, head_dim)
            attention_mask: Optional mask (batch, 1, seq_q, seq_k)
            is_causal: Whether to apply causal masking

        Returns:
            Output (batch, num_heads, seq_q, head_dim)
        """
        batch, num_heads, seq_q, head_dim = q.shape
        _, num_kv_heads, seq_k, _ = k.shape

        # Expand KV for GQA
        if num_kv_heads != num_heads:
            k = self._expand_kv(k, self.num_queries_per_kv)
            v = self._expand_kv(v, self.num_queries_per_kv)

        return scaled_dot_product_attention(
            q, k, v, attention_mask, is_causal, self.scale
        )

    def _expand_kv(self, x: cp.ndarray, num_repeats: int) -> cp.ndarray:
        """Expand KV heads for GQA using broadcast (V6: zero-copy)."""
        batch, num_kv_heads, seq_len, head_dim = x.shape

        # V6: Use expand + reshape for zero-copy expansion
        # (batch, num_kv_heads, seq_len, head_dim)
        # -> (batch, num_kv_heads, 1, seq_len, head_dim)
        # -> broadcast to (batch, num_kv_heads, num_repeats, seq_len, head_dim)
        # -> reshape to (batch, num_kv_heads * num_repeats, seq_len, head_dim)

        x_expanded = cp.broadcast_to(
            x[:, :, None, :, :],
            (batch, num_kv_heads, num_repeats, seq_len, head_dim)
        )
        # Reshape is a view operation when possible
        return x_expanded.reshape(batch, num_kv_heads * num_repeats, seq_len, head_dim)


def next_power_of_two(x: int) -> int:
    """Return the smallest power of two >= x."""
    return 1 << (x - 1).bit_length() if x > 0 else 1


# Maximum dimension for CuTile attention kernel
MAX_ATTENTION_DIM = 256


def scaled_dot_product_attention(
    q: cp.ndarray,
    k: cp.ndarray,
    v: cp.ndarray,
    attention_mask: Optional[cp.ndarray] = None,
    is_causal: bool = False,
    scale: Optional[float] = None
) -> cp.ndarray:
    """
    Scaled dot-product attention using pure CuTile.

    Args:
        q: Query (batch, num_heads, seq_q, head_dim)
        k: Key (batch, num_heads, seq_k, head_dim)
        v: Value (batch, num_heads, seq_k, head_dim)
        attention_mask: Optional additive mask
        is_causal: Whether to apply causal masking
        scale: Optional custom scale factor

    Returns:
        Attention output (batch, num_heads, seq_q, head_dim)
    """
    batch, num_heads, seq_q, head_dim = q.shape
    _, num_kv_heads, seq_k, _ = k.shape

    if scale is None:
        scale = 1.0 / np.sqrt(head_dim)

    # Check if dimensions fit CuTile requirements
    seq_k_padded = next_power_of_two(seq_k)
    head_dim_padded = next_power_of_two(head_dim)

    use_cutile = (
        seq_k_padded <= MAX_ATTENTION_DIM and
        head_dim_padded <= MAX_ATTENTION_DIM
    )

    if use_cutile:
        # Reshape to (batch*heads, seq, dim)
        q_flat = q.reshape(batch * num_heads, seq_q, head_dim).astype(cp.float32)
        k_flat = k.reshape(batch * num_heads, seq_k, head_dim).astype(cp.float32)
        v_flat = v.reshape(batch * num_heads, seq_k, head_dim).astype(cp.float32)

        # Pad to power of 2 if needed
        if seq_k_padded != seq_k or head_dim_padded != head_dim:
            k_padded = cp.zeros((batch * num_heads, seq_k_padded, head_dim_padded), dtype=cp.float32)
            k_padded[:, :seq_k, :head_dim] = k_flat
            k_flat = k_padded

            v_padded = cp.zeros((batch * num_heads, seq_k_padded, head_dim_padded), dtype=cp.float32)
            v_padded[:, :seq_k, :head_dim] = v_flat
            v_flat = v_padded

            q_padded = cp.zeros((batch * num_heads, seq_q, head_dim_padded), dtype=cp.float32)
            q_padded[:, :, :head_dim] = q_flat
            q_flat = q_padded

        scores = cp.empty((batch * num_heads, seq_q, seq_k_padded), dtype=cp.float32)
        output = cp.empty((batch * num_heads, seq_q, head_dim_padded), dtype=cp.float32)

        # Compute scores
        ct.launch(
            get_stream(),
            (batch * num_heads, seq_q),
            attention_scores_kernel,
            (q_flat, k_flat, scores, float(scale), seq_k_padded, head_dim_padded)
        )

        # Mask out padded positions
        if seq_k_padded != seq_k:
            scores[:, :, seq_k:] = -1e9

        # Causal mask
        if is_causal:
            mask = cp.triu(cp.ones((seq_q, seq_k_padded), dtype=cp.float32), k=1) * -1e9
            scores = scores + mask[None, :, :]

        # Attention mask
        if attention_mask is not None:
            if attention_mask.ndim == 4:
                attention_mask = attention_mask.reshape(batch * num_heads, seq_q, seq_k)
            # Pad attention mask
            if seq_k_padded != seq_k:
                mask_padded = cp.zeros((batch * num_heads, seq_q, seq_k_padded), dtype=cp.float32)
                mask_padded[:, :, :seq_k] = attention_mask
                mask_padded[:, :, seq_k:] = -1e9
                attention_mask = mask_padded
            scores = scores + attention_mask

        # Softmax
        ct.launch(
            get_stream(),
            (batch * num_heads, seq_q),
            softmax_inplace_kernel,
            (scores, seq_k_padded)
        )

        # Output
        ct.launch(
            get_stream(),
            (batch * num_heads, seq_q),
            attention_output_kernel,
            (scores, v_flat, output, seq_k_padded, head_dim_padded)
        )

        # Extract actual output
        if head_dim_padded != head_dim:
            output = output[:, :, :head_dim]

        return output.reshape(batch, num_heads, seq_q, head_dim).astype(q.dtype)

    else:
        # Fallback to CuPy for large dimensions
        # scores = Q @ K^T / sqrt(d)
        scores = cp.einsum('bnqd,bnkd->bnqk', q, k) * scale

        # Causal mask
        if is_causal:
            mask = cp.triu(cp.ones((seq_q, seq_k), dtype=cp.float32), k=1) * -1e9
            scores = scores + mask[None, None, :, :]

        # Attention mask
        if attention_mask is not None:
            scores = scores + attention_mask

        # Softmax
        scores = scores - cp.max(scores, axis=-1, keepdims=True)
        exp_scores = cp.exp(scores)
        attn_weights = exp_scores / cp.sum(exp_scores, axis=-1, keepdims=True)

        # Output
        output = cp.einsum('bnqk,bnkd->bnqd', attn_weights, v)

        return output.astype(q.dtype)


# ============================================================================
# Test
# ============================================================================

if __name__ == "__main__":
    print("Testing Pure CuTile Attention...")

    batch_size = 2
    num_heads = 4
    seq_len = 16
    head_dim = 64

    # Create random Q, K, V
    q = cp.random.randn(batch_size, num_heads, seq_len, head_dim).astype(cp.float32)
    k = cp.random.randn(batch_size, num_heads, seq_len, head_dim).astype(cp.float32)
    v = cp.random.randn(batch_size, num_heads, seq_len, head_dim).astype(cp.float32)

    # Test basic attention
    print("\nBasic attention:")
    output = scaled_dot_product_attention(q, k, v)
    print(f"  Output shape: {output.shape}")

    # Test causal attention
    print("\nCausal attention:")
    output_causal = scaled_dot_product_attention(q, k, v, is_causal=True)
    print(f"  Output shape: {output_causal.shape}")

    # Test with attention mask
    print("\nWith attention mask:")
    mask = cp.zeros((batch_size, num_heads, seq_len, seq_len), dtype=cp.float32)
    mask[:, :, :, seq_len // 2:] = -1e9  # Mask out second half
    output_masked = scaled_dot_product_attention(q, k, v, attention_mask=mask)
    print(f"  Output shape: {output_masked.shape}")

    # Test GQA
    print("\nGrouped Query Attention (GQA):")
    num_kv_heads = 2
    k_gqa = cp.random.randn(batch_size, num_kv_heads, seq_len, head_dim).astype(cp.float32)
    v_gqa = cp.random.randn(batch_size, num_kv_heads, seq_len, head_dim).astype(cp.float32)
    attn = MultiHeadAttention(
        hidden_size=num_heads * head_dim,
        num_heads=num_heads,
        num_kv_heads=num_kv_heads
    )
    output_gqa = attn(q, k_gqa, v_gqa)
    print(f"  Output shape: {output_gqa.shape}")

    # Verify output is reasonable
    print("\nOutput statistics:")
    print(f"  Mean: {float(cp.mean(output)):.4f}")
    print(f"  Std:  {float(cp.std(output)):.4f}")
    print(f"  Min:  {float(cp.min(output)):.4f}")
    print(f"  Max:  {float(cp.max(output)):.4f}")

    print("\nPure CuTile Attention working!")
